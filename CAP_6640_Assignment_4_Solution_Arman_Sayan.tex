\documentclass[10pt]{article}
\usepackage{url,hyperref}
%\usepackage{times}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, xparse}

\begin{document}

\noindent \textbf{CAP 6640 -- Natural Language Processing\hspace*{\fill}Spring 2025}\\
\noindent{\bf Homework \#4} \hfill Due date: March 6, 2025

%\vspace*{-0.1in}\paragraph{Instructions:}
%Individual work. Cite all references. All submitted assignments must be typed. Using Latex is required. For \LaTeX, you may use your installation, or online IDEs (no installation required); e.g.,  \href{https://www.overleaf.com/}{www.overleaf.com}. Late submissions by at most 24 hours will be scaled down to 50\%; late beyond 24 hours will be worth 0\%. Total of 40 points (+2 bonus). 
\begin{description}
\item[Problem 1:] \hfill Describe how bidirectional RNNs work, including a diagram and mathematical formulation, and explain the problems they address.

\pagebreak

\item[Problem 2:] \hfill %In the context of CNNs, explain the following concepts with examples: convolution, padding, channels, max pooling, average over time, striding, and k-max pooling. 
 
    \begin{enumerate}
        \item \textbf{Convolution} is the mathematical process of applying a kernel to an input to extract semantic or syntactic features from the given text.
        It involves element-wise multiplication followed by summation. In the context of NLP, convolutions are applied to word embeddings to detect n-gram patterns.
        For example, consider the sentence: "The concert was awesome.". First of all, we embed words into vectors:

        \begin{center}
            "The" $\rightarrow$ [0.1, 0.2, 0.3] \\
            "concert" $\rightarrow$ [0.2, 0.3, 0.4] \\
            "was" $\rightarrow$ [0.3, 0.4, 0.5] \\
            "awesome" $\rightarrow$ [0.4, 0.5, 0.6]
        \end{center}

        Let's assume that we have a kernel of size 2, which is applied to the sentence. The kernel is represented as:

        \begin{center}
            $\displaystyle{\begin{bmatrix} 0.1 & 0.2 & 0.3 \\  0.3 & 0.4 & 0.5 \\ \end{bmatrix}}$
        \end{center}

        The convolution operation is performed as follows:

        \begin{center}
            [0.1, 0.2, 0.3] $\cdot$ [0.1, 0.2, 0.3] + [0.2, 0.3, 0.4] $\cdot$ [0.3, 0.4, 0.5] = 0.1 $\times$ 0.1 + 0.2 $\times$ 0.2 + 0.3 $\times$ 0.3 + 0.2 $\times$ 0.3 + 0.3 $\times$ 0.4 + 0.4 $\times$ 0.5 = 0.01 + 0.04 + 0.09 + 0.06 + 0.12 + 0.2 = 0.52
        \end{center}

        \item \textbf{Padding} involves adding extra zeros or other relevant values around the input to control the output size and preserve sequence length in text.
        For example, without padding the sentence "The concert was awesome.", applying a kernel size of 3 reduces output length from 4 words to 2 words.
        With zero padding, we maintain the original length of the sentence as follows:

        \begin{center}
            "$\phi$" $\rightarrow$ [0.0, 0.0, 0.0] \\
            "The" $\rightarrow$ [0.1, 0.2, 0.3] \\
            "concert" $\rightarrow$ [0.2, 0.3, 0.4] \\
            "was" $\rightarrow$ [0.3, 0.4, 0.5] \\
            "awesome" $\rightarrow$ [0.4, 0.5, 0.6] \\
            "$\phi$" $\rightarrow$ [0.0, 0.0, 0.0] \\
        \end{center}

        \item \textbf{Channels} are the number of feature maps generated by applying multiple kernels to the input. 
        Each channel represents a different feature extracted from the text. This allows to combine different linguistic features.
        For example, if we apply 2 kernels to the sentence "The concert was awesome.", we get 2 channels, each representing a different feature.
        Assume that the other kernel is as follows:

        \begin{center}
            $\displaystyle{\begin{bmatrix} 0.2 & 0.1 & 0.3 \\  0.4 & 0.3 & 0.5 \\ \end{bmatrix}}$
        \end{center}

        For the bigram "The concert", after applying the two kernels, we get two channels for representation as follows:

        \begin{center}
            "The", "concert" $\rightarrow$ [0.52, 0.50] \\
        \end{center}

        \item \textbf{Max Pooling} is a downsampling operation that selects the maximum value from a set of values in a given window while preserving key information.
        This operation helps to capture the most important features. For example, assume that we have the following feature map:

        \begin{center}
            "$\phi$", "The", "concert" $\rightarrow$ [0.52, 0.50] \\
            "The", "concert", "was" $\rightarrow$ [0.48, 0.56] \\
            "concert", "was", "awesome" $\rightarrow$ [0.64, 0.42] \\
            "was", "awesome", "$\phi$" $\rightarrow$ [0.40, 0.38]
        \end{center}

        After applying max pooling for each channel seperately, we get the following representation:

        \begin{center}
            max p $\rightarrow$ [0.64, 0.56] \\
        \end{center}

        \item \textbf{Average Over Time} is a pooling operation that calculates the average of all feature activations across the sequence to capture the overall context.
        This operation helps to reduce the dimensionality of the feature map. Furthermore, averaging them might provide a sentence-level embedding.
        For example, assume that we have the following feature map:

        \begin{center}
            "$\phi$", "The", "concert" $\rightarrow$ [0.52, 0.50] \\
            "The", "concert", "was" $\rightarrow$ [0.48, 0.56] \\
            "concert", "was", "awesome" $\rightarrow$ [0.64, 0.42] \\
            "was", "awesome", "$\phi$" $\rightarrow$ [0.40, 0.38]
        \end{center}

        After applying average over time for each channel seperately, we get the following representation:

        \begin{center}
            avg $\rightarrow$ [0.51, 0.465] \\
        \end{center}

        \item \textbf{Striding} is the process of moving the kernel by a certain number of steps across the input duing convolution to reduce the output size.
        This helps to control computational efficiency and feature map size. When we select stride as 1, the kernel moves one step at a time which outputs a dense feature map.
        For example, assume that we get the following feature map when we apply kernel size of 2 and stride of 1 to the sentence "The concert was awesome.":

        \begin{center}
            "$\phi$", "The", "concert" $\rightarrow$ [0.52, 0.50] \\
            "The", "concert", "was" $\rightarrow$ [0.48, 0.56] \\
            "concert", "was", "awesome" $\rightarrow$ [0.64, 0.42] \\
            "was", "awesome", "$\phi$" $\rightarrow$ [0.40, 0.38]
        \end{center}

        For stride of 2 with the same kernel size, the feature map would be as follows:

        \begin{center}
            "$\phi$", "The", "concert" $\rightarrow$ [0.52, 0.50] \\
            "concert", "was", "awesome" $\rightarrow$ [0.64, 0.42]
        \end{center}

        \item \textbf{K-Max Pooling} selects the top k largest activations values from the feature map instead of a fixed window. 
        This retains significant features while preserving the order of the sequence. For example, assume that we have the following feature map:

        \begin{center}
            "$\phi$", "The", "concert" $\rightarrow$ [0.52, 0.50] \\
            "The", "concert", "was" $\rightarrow$ [0.48, 0.56] \\
            "concert", "was", "awesome" $\rightarrow$ [0.64, 0.42] \\
            "was", "awesome", "$\phi$" $\rightarrow$ [0.40, 0.38]
        \end{center}

        After applying k-max pooling for each channel seperately with k=2, we get the following representation:

        \begin{center}
            k-max p $\displaystyle{\rightarrow \begin{bmatrix} 0.52 & 0.50 \\  0.64 & 0.56 \\ \end{bmatrix}}$
        \end{center}

    \end{enumerate}

\pagebreak

\item[Problem 3:] \hfill %Discuss the operation of multi-layer RNNs, highlighting the advantages of using multiple layers and identifying four key challenges associated with them.

A multi-layer RNN consist of multiple stacked RNN layers where the output of one layer is fed as input to the next layer.
This hierarchical architecture allows the network to learn complex patterns and dependencies in the data across different levels of abstraction.

For an in input sequence $X = (x_1, x_2, ..., x_T)$, a single-layer RNN updates its hidden state as:

\begin{center}
    $h_t^{(1)} = f(W_{h}h_{t-1}^{(1)} + b_h)$
\end{center}

where $h_t^{(1)}$ is the hidden state of the first layer at time step $t$, $W_{h}$ is the weight matrix, $b_h$ is the bias term, and $f$ is the activation function.

What multi-layer RNNs do is to extend this formulation to multiple layers. In a multi-layer RNN, each hidden state at layer $l$ takes input from the previous layer's hidden state as

\begin{center}
    $h_t^{(l)} = f(W_{h}^{(l)}h_t^{(l-1)} + b_h^{(l)})$
\end{center}

where $h_t^{(l)}$ is the hidden state of the $l$-th layer at time step $t$, $W_{h}^{(l)}$ is the weight matrix, $b_h^{(l)}$ is the bias term, and $f$ is the activation function.

This enhances representation learning, allowing the network to learn hierarchical representations of the input data, where each layer processes information at a different level of abstraction, with lower layers 
capturing local patterns and deeper layers modeling high-level dependencies.

The advantages of using multiple layers in RNNs include:

\begin{enumerate}
    \item \textbf{Hierarchical Representation Learning:} As mentioned earlier, multi-layer RNNs can learn hierarchical representations of the input data.
    Lower layers capture local patterns like words in a phrase, while deeper layers capture long-term dependencies like sentence meaning.
    \item \textbf{Increased Model Capacity:} Compared to single layer RNNs, multi-layer RNNs can model complex sequential data better than shallow alternatives.
    \item \textbf{Improved Generalization:} Hierarchical feature extraction of multi-layer RNNs makes the useful for complex sequences like a long text and speech.
\end{enumerate}

However, multi-layer RNNs also face several challenges:

\begin{enumerate}
    \item \textbf{Vanishing and Exploding Gradients:} Like in every RNN architecture, as gradients propagate through many layers and time steps, 
    they become exponentially small, preventing earlier layers from learning.
    \item \textbf{Long-Term Dependency:} Even though multi-layer RNNs are better than shallow ones, they still struggle to capture long-term dependencies in the data due to 
    the vanishing gradient problem, which can limit their ability to model sequences with long-range dependencies.
    \item \textbf{Computational Complexity and Cost:} Having multiple RNNs means more model parameters to train, which can require significant computational resources, 
    making training slow.
    \item \textbf{Optimization Challenges:} Training multi-layer RNNs can be challenging due to overfitting and convergence of gradients, which can make it harder to 
    find an optimal model configuration.
\end{enumerate}

\pagebreak

\item[Problem 4:] \hfill Explain the encoder-decoder architecture, detailing its working mechanism and providing three real-world applications.  

\pagebreak

\item[Problem 5:] \hfill Explain how CNNs are applied to text understanding tasks, such as sentiment analysis.

\pagebreak

\item[Problem 6:] \hfill Describe how neural machine translation operates and compare it to alternative approaches.  

\pagebreak

\item[Problem 7:] \hfill Identify three limitations of RNNs that CNNs overcome.  

\end{description}

\end{document}